---
title: "GP-summer-practice2"
format: html
editor: visual
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(readr)
library(readxl)
library(easystats)
library(psych)
library(lubridate)
library(tidyr)
library(lme4)
library(kableExtra)
library(sparkline)
library(marginaleffects)
library(see)
library(formattable)
library(magrittr)
library(repurrrsive)
library(broom)
```

```{r}
#| label: load data
GP <- read_csv("GP_Data_das.csv", show_col_types = FALSE)

GP['PID'][GP['PID'] == 518806] <- 244
# For labeling purposes
```

## Question 1

1.  How many people are in the dataset?

```{r}
n_people <- GP |>
  summarize(
    people = n_distinct(PID)
  )
```

There are `r n_people` people.

## Question 2

2.  How many values in total do we have? What is the % missing?

```{r}
values_num <- function(df, vars) {
  df |>
    summarize(
      values_here = sum(!is.na({{ vars }})),
      values_miss = sum(is.na({{ vars }}))
    )
}

values_table <- GP |>
  values_num(across(everything())) |>
  mutate(
      values_total = values_here + values_miss
  )
```

There are `r values_table$values_total` total values.

```{r}
values_NA <- GP |>
  values_num(across(everything())) |>
  mutate(
    values_total = values_here + values_miss
  ) |> 
  summarize(
    values_miss = values_miss / values_total
  )
```

`r values_NA$values_miss` are missing values.

## Question 3

3.  How many observations per person? What is the average, SD, and range of \# of observations?

```{r}
GP <- GP |>
  group_by(PID) |>
  mutate(
    obs_total = n(), 
    .after = PID
  )

GP |>
  group_by(PID) |>
  distinct(obs_total) |> 
  print()
```

```{r}
obs_average <- mean(GP$obs_total)
obs_sd <- sd(GP$obs_total)
obs_range <- max(GP$obs_total) - min(GP$obs_total)
```

Average = `r obs_average`; SD = `r obs_sd`; Range = `r obs_range`

## Question 4

4.  Calculate a correlation matrix for the entire dataset (I like the correlation package from the easystats suite of packages but you can do this many many different ways).

```{r}
GP_cor1 <- GP |>
subset(
    select = c(E1_1:O3_3, Extraversion:Openness)
    )

GP_matrix1 <- correlation::correlation(GP_cor1)

GP_matrix1 |>
  summary(redundant = TRUE) |>
  plot()
```

## Question 5

5.Create reverse scored new items and name them with an \_r tacked onto the variable e.g., E2_3_r. Take a look at the correlation matrix to figure out what to reverse score. Rerun 4 with the reverse scored items instead of the non-reverse scored items. (I like to use psych package for the reverse coding)

E3_3 and A1_3 are reverse coded

```{r}
reverse_cols = c("E3_3", "A1_3")
GP[ , reverse_cols] = 6 - GP[ , reverse_cols]

GP <- GP |>
  rename(
    E3_3_r = E3_3,
    A1_3_r = A1_3
  )

GP_cor2 <- GP |>
subset(
    select = c(E1_1:O3_3, Extraversion:Openness))

GP_matrix2 <- correlation::correlation(GP_cor2)

GP_matrix2 |>
  summary(redundant = TRUE) |>
  plot()
```

## Question 6

6.  Create composites of each facet and Big Five factor.

I needed to create the wave variable here to group by. When I grouped by PID without wave it averaged across every time they answered the item and I could not calculate correlation coefficient's for Question #9.

```{r}
#| message: false
#| warning: false

GP <- GP |>
  group_by(PID) |>
  mutate(
    wave = 1:n(),
    .after = obs_total
  )

GP_comp1 <- GP |>
  select(PID, wave, E1_1:O3_3) |>
pivot_longer(
    cols = matches("[eacno][1-3]_[1-3]|_[r]"),
    names_to = "item",
    values_to = "item_scores",
  ) |>
  mutate(
    facet = case_when(
      startsWith(item, "E1") ~ "E1",
      startsWith(item, "E2") ~ "E2",
      startsWith(item, "E3") ~ "E3",
      startsWith(item, "A1") ~ "A1",
      startsWith(item, "A2") ~ "A2",
      startsWith(item, "A3") ~ "A3",
      startsWith(item, "C1") ~ "C1",
      startsWith(item, "C2") ~ "C2",
      startsWith(item, "C3") ~ "C3",
      startsWith(item, "N1") ~ "N1",
      startsWith(item, "N2") ~ "N2",
      startsWith(item, "N3") ~ "N3",
      startsWith(item, "O1") ~ "O1",
      startsWith(item, "O2") ~ "O2",
      startsWith(item, "O3") ~ "O3",
    )
  ) |>
  mutate(
    trait = case_when(
      startsWith(facet, "E") ~ "Extraversion",
      startsWith(facet, "A") ~ "Agreeableness",
      startsWith(facet, "C") ~ "Conscientiousness",
      startsWith(facet, "N") ~ "Neuroticism",
      startsWith(facet, "O") ~ "Openness"
    )
  )

GP_facets <- GP_comp1 |>
  group_by(PID, wave, facet) |>
    summarize(
    facet_scores = mean(item_scores, na.rm = TRUE)
    ) |>
  pivot_wider(
    names_from = facet,
    values_from = facet_scores
  )

GP_traits <- GP_comp1 |>
  group_by(PID, wave, trait) |>
  summarize(
    trait_scores = mean(item_scores, na.rm = TRUE)
  ) |>
  pivot_wider(
    names_from = trait,
    values_from = trait_scores
  )

GP_comp2 <- GP_facets |>
  left_join(GP_traits) |>
  print()
```

## Question 7

7.  Calculate a correlation matrix for these composites.

```{r}
GP_comp3 <- GP_comp2 |>
  subset(
    select = c(-PID, -wave)
  )

comp_matrix <- correlation::correlation(GP_comp3)

comp_matrix |>
  summary(redundant = TRUE) |>
  plot()
```

## Question 8

8.  What is the reliability of each facet? And of each big five factor? (Psych pagackage again, though the performance package from easystats might be the way to go as that package has more functions that are very useful so maybe it is worth just doing it all within that package?).

I tried using the pivoting from Question 6 but it wasn't working right with the alpha function because it seems like it wants each item value in a separate column, and after pivoting wider I couldn't group by facet or trait.

```{r}
facet_alphas <- data.frame(facet = c("E1", "E2", "E3", "A1", "A2", "A3",
                                 "C1", "C2", "C3", "N1", "N2", "N3",
                                 "O1", "O2", "O3"))

facet_alphas$alpha[[1]] <- alpha(GP[ , c("E1_1", "E1_2", "E1_3")])[[1]][[1]]
facet_alphas$alpha[[2]] <- alpha(GP[ , c("E2_1", "E2_2", "E2_3")])[[1]][[1]]
facet_alphas$alpha[[3]] <- alpha(GP[ , c("E3_1", "E3_2", "E3_3_r")])[[1]][[1]]

facet_alphas$alpha[[4]] <- alpha(GP[ , c("A1_1", "A1_2", "A1_3_r")])[[1]][[1]]
facet_alphas$alpha[[5]] <- alpha(GP[ , c("A2_1", "A2_2", "A2_3")])[[1]][[1]]
facet_alphas$alpha[[6]] <- alpha(GP[ , c("A3_1", "A3_2", "A3_3")])[[1]][[1]]

facet_alphas$alpha[[7]] <- alpha(GP[ , c("C1_1", "C1_2", "C1_3")])[[1]][[1]]
facet_alphas$alpha[[8]] <- alpha(GP[ , c("C2_1", "C2_2", "C2_3")])[[1]][[1]]
facet_alphas$alpha[[9]] <- alpha(GP[ , c("C3_1", "C3_2", "C3_3")])[[1]][[1]]

facet_alphas$alpha[[10]] <- alpha(GP[ , c("N1_1", "N1_2", "N1_3")])[[1]][[1]]
facet_alphas$alpha[[11]] <- alpha(GP[ , c("N2_1", "N2_2", "N2_3")])[[1]][[1]]
facet_alphas$alpha[[12]] <- alpha(GP[ , c("N3_1", "N3_2", "N3_3")])[[1]][[1]]

facet_alphas$alpha[[13]] <- alpha(GP[ , c("O1_1", "O1_2", "O1_3")])[[1]][[1]]
facet_alphas$alpha[[14]] <- alpha(GP[ , c("O2_1", "O2_2", "O2_3")])[[1]][[1]]
facet_alphas$alpha[[15]] <- alpha(GP[ , c("O3_1", "O3_2", "O3_3")])[[1]][[1]]

facet_alphas <- facet_alphas |>
  pivot_wider(
    names_from = facet,
    values_from = alpha
  )

trait_alphas <- data.frame(trait = c("Extraversion", "Agreeableness", 
                                     "Conscientiousness", "Neuroticism",
                                     "Openness"))
trait_alphas$alpha[[1]] <- alpha(GP[ , c("E1_1", "E1_2", "E1_3", "E2_1", "E2_2", 
                                         "E2_3", "E3_1", "E3_2", 
                                         "E3_3_r")])[[1]][[1]]
trait_alphas$alpha[[2]] <- alpha(GP[ , c("A1_1", "A1_2", "A1_3_r", "A2_1", "A2_2", 
                                         "A2_3", "A3_1", "A3_2", 
                                         "A3_3")])[[1]][[1]]
trait_alphas$alpha[[3]] <- alpha(GP[ , c("C1_1", "C1_2", "C1_3", "C2_1", "C2_2", 
                                         "C2_3", "C3_1", "C3_2", 
                                         "C3_3")])[[1]][[1]]
trait_alphas$alpha[[4]] <- alpha(GP[ , c("N1_1", "N1_2", "N1_3", "N2_1", "N2_2", 
                                         "N2_3", "N3_1", "N3_2", 
                                         "N3_3")])[[1]][[1]]
trait_alphas$alpha[[5]] <- alpha(GP[ , c("O1_1", "O1_2", "O1_3", "O2_1", "O2_2", 
                                         "O2_3", "O3_1", "O3_2", 
                                         "O3_3")])[[1]][[1]]

trait_alphas <- trait_alphas |>
  pivot_wider(
    names_from = trait,
    values_from = alpha
  )

GP_alphas <- facet_alphas |>
  cross_join(trait_alphas)

GP_alphas <- GP_alphas |>
  unnest() |>
  print()
```

## Question 9

9.  Pick two facets in different traits to compare. Calculate the correlation coefficient at the *PERSON* level for every participant in the dataset. Graph the distribution (density or histogram) of these correlations.

```{r}
GP_cor <- GP_comp2 |>
  group_by(PID) |>
  summarize(cor = cor(E1, O2))

ggplot(GP_cor, aes(x = cor)) +
  geom_density()
```

## Question 10

10. Create a new variable that indexes the number of times a person has taken the survey (e.g,. A wave variable).

```{r}
#| results: hide

GP |>
  group_by(PID) |>
  mutate(
    wave = 1:n(),
    .after = PID
  )
```

## Question 11

11. Graph the relationship between time and one of the big five for each person in a separate pane (i.e., group by ID in ggplot and use a smooth function), plus include the raw data.

```{r}
GP |>
  group_by(PID) |> 
  ggplot(aes(x = wave, y = Conscientiousness)) +
  geom_smooth() +
  facet_wrap(~PID)
```

## Question 12

12. Run a regression for each person for the same big five factor and time e.g., lm(E \~ time). Create a density plot of people's regression coefficients.

```{r}
model_C <- lmList(Conscientiousness ~ wave | PID, data = GP)

mapped_C <- map_df(model_C, ~.x$coefficients)

ggplot(mapped_C, aes(x = wave)) +
  geom_density()
```

## Question 13

13. Create an html table (using KableExtra) where each person's information is in a row, with columns that describe (ID, #observations from #3, intercept (se), regression (se) from #12, and then use sparkline package to show the observational level line of each person, similar to #11

```{r}
#| message: false
#| warning: false

summary_C <- summary(model_C)

frame_C <- as.data.frame(summary_C$coefficients)

frame_C <- frame_C |>
  mutate(
    across(c(`Estimate.(Intercept)`,`Std. Error.(Intercept)`,
           Estimate.wave, `Std. Error.wave`), round, 9)
  ) |>
  mutate(
    `Intercept_estimate (se)` = paste(`Estimate.(Intercept)`, "(", 
                                      `Std. Error.(Intercept)`, ")"),
    `Regression_estimate (se)` = paste(Estimate.wave, "(", 
                                       `Std. Error.wave`, ")")
  ) |> subset(
    select = c(`Intercept_estimate (se)`, `Regression_estimate (se)`)
  )

join_PID <- GP |>
  distinct(PID)

frame_C$PID <- join_PID$PID

frame_C <- frame_C |>
  relocate(
    PID, 
    .before = `Intercept_estimate (se)`
  )

GP_spark <- GP |>
  group_by(PID) |>
  select(PID, obs_total, Conscientiousness)

GP_spark <- GP_spark |>
  group_by(PID, obs_total) |>
    summarize(
    TrendSparkline = spk_chr(
      Conscientiousness, type = "line"
    )
  )

GP_spark <- GP_spark |>
  full_join(frame_C, join_by(PID))

GP_spark <- GP_spark[, c("PID", "obs_total", "Intercept_estimate (se)", 
                         "Regression_estimate (se)", "TrendSparkline")]

GP_spark <- GP_spark |>
  formattable() |>
  as.htmlwidget() |>
  spk_add_deps()

GP_spark
```

## Question 14

14. Create model derived "predicted values" of the regression model based on original data (i.e fitted or Y-hat values). Use these to then plot the regression lines for each person plus an average regression line all within a single pane. To do this, I would use the marginal effects package, and then with the predicted values id graph them in ggplot using geom_line

I know you said I would have to calculate the average regression line separately, but I was thinking that geom_smooth() plotted a line that already did that. Is this not correct, and if so, what is the average line supposed to look like?

```{r}
GP_predict <- predict(model_C)

GP$fitted_values <- GP_predict

GP |>
  ggplot(aes(x = wave, y = fitted_values)) +
  geom_line(aes(group = PID, color = PID)) + 
  geom_smooth(color = "red", se = FALSE)
```

## Question 15

15. The regression, graphing, and table building all centered on a single model. Without necessarily doing it, how would you set it up to iterate through all possible regressions, graphs, and tables?

I found a great video on YouTube working with nested data frames to map linear models as separate variables. I was able to get each regression grouped by PID, based on one model (Conscientiousness \~ wave) and make the two plots from the previous questions.

For creating an html table, all of the estimates and ses were already in the data frame, but I was having trouble extracting obs_total, wave, and Conscientiousness because of the nested data, and unnesting each grouped data frame was not possible for my computer. Instead, I put the Question #13 code into a function (this time, one that works).

```{r}
#| message: false

GP_nested <- GP |>
  group_by(PID) |>
  nest()

function_lm <- function(df) {
    lm(Conscientiousness ~ wave, data = df)
}

function_fitted <- function(df) {
  predict(lm(Conscientiousness ~ wave, data = df))
}

GP_nested <- GP_nested |>
  mutate(
    model = map(data, function_lm),
    model_tidy = map(model, tidy),
    fitted_model = map(data, function_fitted),
    fitted_tidy = map(fitted_model, tidy)
  )

GP_unnest <- GP_nested |>
  unnest(model_tidy) |>
  unnest(fitted_tidy)

GP_wide <- GP_unnest |>
  pivot_wider(
    names_from = term,
    values_from = estimate:p.value
  ) |> mutate(
    names = as.numeric(names)
  )

GP_wide |>
  ggplot(aes(x = estimate_wave)) +
  geom_density()

GP_wide |>
  ggplot(aes(x = names, y = x)) +
  geom_line(aes(group = PID, color = PID)) +
  geom_smooth(color = "red", se = FALSE)

spark_table <- function(GP_wide, GP) {

stats_frame <- GP_wide |>
    group_by(PID) |>
    subset(
      select = c(PID, `estimate_(Intercept)`:`std.error_wave`)
    )

stats_frame <- stats_frame |>
  distinct()

stats_frame <- stats_frame |>
  mutate(
    across(c(`estimate_(Intercept)`,`std.error_(Intercept)`,
           estimate_wave, `std.error_wave`), round, 9)
  ) |>
  mutate(
    `Intercept_estimate (se)` = paste(`estimate_(Intercept)`, "(", 
                                      `std.error_(Intercept)`, ")"),
    `Regression_estimate (se)` = paste(estimate_wave, "(", 
                                       `std.error_wave`, ")")
  ) |> subset(
    select = c(PID, `Intercept_estimate (se)`, `Regression_estimate (se)`)
  )

spark_frame <- GP |>
  group_by(PID) |>
  select(PID, obs_total, Conscientiousness)

spark_frame <- spark_frame |>
  group_by(PID, obs_total) |>
    summarize(
    TrendSparkline = spk_chr(
      Conscientiousness, type = "line"
    )
  )

spark_frame <- spark_frame |>
  full_join(stats_frame, join_by(PID))

spark_frame <- spark_frame |>
  formattable() |>
  as.htmlwidget() |>
  spk_add_deps()

return(spark_frame)

}

spark_table(GP_wide, GP)
```
