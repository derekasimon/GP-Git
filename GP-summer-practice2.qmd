---
title: "GP-summer-practice2"
format: html
editor: visual
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(readr)
library(readxl)
library(easystats)
library(psych)
library(lubridate)
library(tidyr)
library(lme4)
library(kableExtra)
library(sparkline)
library(marginaleffects)
library(see)
library(formattable)
library(magrittr)
```

```{r}
#| label: load data
GP <- read_csv("GP_Data_das.csv", show_col_types = FALSE)
```

## Question 1

1.  How many people are in the dataset?

```{r}
n_people <- GP |>
  summarize(
    people = n_distinct(PID)
  )
```

There are `r n_people` people.

## Question 2

2.  How many values in total do we have? What is the % missing?

```{r}
values_num <- function(df, vars) {
  df |>
    summarize(
      values_here = sum(!is.na({{ vars }})),
      values_miss = sum(is.na({{ vars }}))
    )
}

values_table <- GP |>
  values_num(across(everything())) |>
  mutate(
      values_total = values_here + values_miss
  )
```

There are `r values_table$values_total` total values.

```{r}
values_NA <- GP |>
  values_num(across(everything())) |>
  mutate(
    values_total = values_here + values_miss
  ) |> 
  summarize(
    values_miss = values_miss / values_total
  )
```

`r values_NA$values_miss` are missing values.

## Question 3

3.  How many observations per person? What is the average, SD, and range of \# of observations?

```{r}
GP <- GP |>
  group_by(PID) |>
  mutate(
    obs_total = n(), 
    .after = PID
  )
```

There are `r GP$obs_total` total observations.

```{r}
obs_average <- mean(GP$obs_total)
obs_sd <- sd(GP$obs_total)
obs_range <- max(GP$obs_total) - min(GP$obs_total)
```

Average = `r obs_average`; SD = `r obs_sd`; Range = `r obs_range`

## Question 4

4.  Calculate a correlation matrix for the entire dataset (I like the correlation package from the easystats suite of packages but you can do this many many different ways).

```{r}
GP_cor1 <- GP |>
subset(
    select = c(E1_1:O3_3, Extraversion:Openness)
    )

GP_matrix1 <- correlation::correlation(GP_cor1)

GP_matrix1 |>
  summary(redundant = TRUE) |>
  plot()
```

## Question 5

5.Create reverse scored new items and name them with an \_r tacked onto the variable e.g., E2_3_r. Take a look at the correlation matrix to figure out what to reverse score. Rerun 4 with the reverse scored items instead of the non-reverse scored items. (I like to use psych package for the reverse coding)

E3_3 and A1_3 are reverse coded

```{r}
reverse_cols = c("E3_3", "A1_3")
GP[ , reverse_cols] = 6 - GP[ , reverse_cols]

GP <- GP |>
  rename(
    E3_3_r = E3_3,
    A1_3_r = A1_3
  )

GP_cor2 <- GP |>
subset(
    select = c(E1_1:O3_3, Extraversion:Openness))

GP_matrix2 <- correlation::correlation(GP_cor2)

GP_matrix2 |>
  summary(redundant = TRUE) |>
  plot()
```

## Question 6

6.  Create composites of each facet and Big Five factor.

```{r}
view(GP)

GP_comp1 <- GP |>
  select(PID, E1_3:O3_3) |>
pivot_longer(
    cols = matches("[eacno][1-3]_[1-3]|_[r]"),
    names_to = "item",
    values_to = "item_scores",
  ) |>
  mutate(
    facet = case_when(
      startsWith(item, "E1") ~ "E1",
      startsWith(item, "E2") ~ "E2",
      startsWith(item, "E3") ~ "E3",
      startsWith(item, "A1") ~ "A1",
      startsWith(item, "A2") ~ "A2",
      startsWith(item, "A3") ~ "A3",
      startsWith(item, "C1") ~ "C1",
      startsWith(item, "C2") ~ "C2",
      startsWith(item, "C3") ~ "C3",
      startsWith(item, "N1") ~ "N1",
      startsWith(item, "N2") ~ "N2",
      startsWith(item, "N3") ~ "N3",
      startsWith(item, "O1") ~ "O1",
      startsWith(item, "O2") ~ "O2",
      startsWith(item, "O3") ~ "O3",
    )
  ) |>
  mutate(
    trait = case_when(
      startsWith(facet, "E") ~ "Extraversion",
      startsWith(facet, "A") ~ "Agreeableness",
      startsWith(facet, "C") ~ "Conscientiousness",
      startsWith(facet, "N") ~ "Neuroticism",
      startsWith(facet, "O") ~ "Openness"
    )
  )

GP_facets <- GP_comp1 |>
  group_by(PID, facet) |>
    summarize(
    facet_scores = mean(item_scores, na.rm = TRUE)
    ) |>
  pivot_wider(
    names_from = facet,
    values_from = facet_scores
  )

GP_traits <- GP_comp1 |>
  group_by(PID, trait) |>
  summarize(
    trait_scores = mean(item_scores, na.rm = TRUE)
  ) |>
  pivot_wider(
    names_from = trait,
    values_from = trait_scores
  )

GP_comp2 <- GP_facets |>
  left_join(GP_traits)
```

## Question 7

7.  Calculate a correlation matrix for these composites.

```{r}
GP_comp3 <- GP_comp2 |>
  subset(
    select = -PID
  )

comp_matrix <- correlation::correlation(GP_comp3)

comp_matrix |>
  summary(redundant = TRUE) |>
  plot()
```

## Question 8

8.  What is the reliability of each facet? And of each big five factor? (Psych pagackage again, though the performance package from easystats might be the way to go as that package has more functions that are very useful so maybe it is worth just doing it all within that package?).

I tried using the pivoting from Question 6 but it wasn't working right with the alpha function because it seems like it wants each item value in a separate column, and with the data frame wider I couldn't group by facet or trait.

```{r}
facet_alphas <- data.frame(facet = c("E1", "E2", "E3", "A1", "A2", "A3",
                                 "C1", "C2", "C3", "N1", "N2", "N3",
                                 "O1", "O2", "O3"))

facet_alphas$alpha[[1]] <- alpha(GP[ , c("E1_1", "E1_2", "E1_3")])[[1]][[1]]
facet_alphas$alpha[[2]] <- alpha(GP[ , c("E2_1", "E2_2", "E2_3")])[[1]][[1]]
facet_alphas$alpha[[3]] <- alpha(GP[ , c("E3_1", "E3_2", "E3_3_r")])[[1]][[1]]

facet_alphas$alpha[[4]] <- alpha(GP[ , c("A1_1", "A1_2", "A1_3_r")])[[1]][[1]]
facet_alphas$alpha[[5]] <- alpha(GP[ , c("A2_1", "A2_2", "A2_3")])[[1]][[1]]
facet_alphas$alpha[[6]] <- alpha(GP[ , c("A3_1", "A3_2", "A3_3")])[[1]][[1]]

facet_alphas$alpha[[7]] <- alpha(GP[ , c("C1_1", "C1_2", "C1_3")])[[1]][[1]]
facet_alphas$alpha[[8]] <- alpha(GP[ , c("C2_1", "C2_2", "C2_3")])[[1]][[1]]
facet_alphas$alpha[[9]] <- alpha(GP[ , c("C3_1", "C3_2", "C3_3")])[[1]][[1]]

facet_alphas$alpha[[10]] <- alpha(GP[ , c("N1_1", "N1_2", "N1_3")])[[1]][[1]]
facet_alphas$alpha[[11]] <- alpha(GP[ , c("N2_1", "N2_2", "N2_3")])[[1]][[1]]
facet_alphas$alpha[[12]] <- alpha(GP[ , c("N3_1", "N3_2", "N3_3")])[[1]][[1]]

facet_alphas$alpha[[13]] <- alpha(GP[ , c("O1_1", "O1_2", "O1_3")])[[1]][[1]]
facet_alphas$alpha[[14]] <- alpha(GP[ , c("O2_1", "O2_2", "O2_3")])[[1]][[1]]
facet_alphas$alpha[[15]] <- alpha(GP[ , c("O3_1", "O3_2", "O3_3")])[[1]][[1]]

facet_alphas <- facet_alphas |>
  pivot_wider(
    names_from = facet,
    values_from = alpha
  )

trait_alphas <- data.frame(trait = c("Extraversion", "Agreeableness", 
                                     "Conscientiousness", "Neuroticism",
                                     "Openness"))
trait_alphas$alpha[[1]] <- alpha(GP[ , c("E1_1", "E1_2", "E1_3", "E2_1", "E2_2", 
                                         "E2_3", "E3_1", "E3_2", 
                                         "E3_3_r")])[[1]][[1]]
trait_alphas$alpha[[2]] <- alpha(GP[ , c("A1_1", "A1_2", "A1_3_r", "A2_1", "A2_2", 
                                         "A2_3", "A3_1", "A3_2", 
                                         "A3_3")])[[1]][[1]]
trait_alphas$alpha[[3]] <- alpha(GP[ , c("C1_1", "C1_2", "C1_3", "C2_1", "C2_2", 
                                         "C2_3", "C3_1", "C3_2", 
                                         "C3_3")])[[1]][[1]]
trait_alphas$alpha[[4]] <- alpha(GP[ , c("N1_1", "N1_2", "N1_3", "N2_1", "N2_2", 
                                         "N2_3", "N3_1", "N3_2", 
                                         "N3_3")])[[1]][[1]]
trait_alphas$alpha[[5]] <- alpha(GP[ , c("O1_1", "O1_2", "O1_3", "O2_1", "O2_2", 
                                         "O2_3", "O3_1", "O3_2", 
                                         "O3_3")])[[1]][[1]]

trait_alphas <- trait_alphas |>
  pivot_wider(
    names_from = trait,
    values_from = alpha
  )

GP_alphas <- facet_alphas |>
  cross_join(trait_alphas) |> view()
```

## Question 9

9.  Pick two facets in different traits to compare. Calculate the correlation coefficient at the *PERSON* level for every participant in the dataset. Graph the distribution (density or histogram) of these correlations.

```{r}
GP_cor <- GP_comp2 |>
  group_by(PID)
  summarize(cor = cor(E1, O2))

ggplot(GP_cor, aes(x = cor)) +
  geom_density()
```

## Question 10

10. Create a new variable that indexes the number of times a person has taken the survey (e.g,. A wave variable).

```{r}
GP <- GP |>
  group_by(PID) |>
  mutate(
    wave = 1:n(),
    .after = PID
  )
```

## Question 11

11. Graph the relationship between time and one of the big five for each person in a separate pane (i.e., group by ID in ggplot and use a smooth function), plus include the raw data.

```{r}
GP |>
  group_by(PID) |>
ggplot(aes(x = wave, y = Conscientiousness)) +
  geom_smooth() +
  facet_wrap(~PID)
```

## Question 12

12. Run a regression for each person for the same big five factor and time e.g., lm(E \~ time). Create a density plot of people's regression coefficients.

I used purrr to extract the coefficients this time, but it took some extra steps to get it into a data frame.

```{r}
model_C <- lmList(Conscientiousness ~ wave | PID, data = GP)

frame_C <- map(model_C, ~.x$coefficients)

frame_C <- do.call(rbind.data.frame, frame_C)

frame_C <- frame_C |>
  rename(
    intercept = c.3.1193510738556..3.26801437562965..2.63425076454128..2.90166975891074..,
    wave = c.0.000460680138888127...0.00362831784195784..0.0105485802734576..
  )

GP_join <- GP |>
  group_by(PID) |>
  distinct(PID)

frame_C$PID <- GP_join$PID

ggplot(frame_C, aes(x = wave)) +
  geom_density()
```

## Question 13

13. Create an html table (using KableExtra) where each person's information is in a row, with columns that describe (ID, #observations from #3, intercept (se), regression (se) from #12, and then use sparkline package to show the observational level line of each person, similar to #11

```{r}
summary_C <- summary(model_C)

GP_obs <- GP |>
  group_by(PID, obs_total) |>
  distinct(PID) |>
  relocate(PID)

GP_obs$intercept_se <- summary_C$coefficients[ , 2, 1]

GP_obs$regression_se <- summary_C$coefficients[ , 2, 2]

GP_spark <- GP |>
  group_by(PID) |>
  select(PID, wave, Conscientiousness)

GP_spark <- GP_spark |>
  group_by(PID) |>
    summarize(
    TrendSparkline = spk_chr(
      Conscientiousness, type = "line"
    )
  )

GP_spark <- GP_spark |>
  full_join(GP_obs, join_by(PID))

GP_spark <- GP_spark[, c("PID", "obs_total", "intercept_se", "regression_se",
                       "TrendSparkline")]

GP_spark <- GP_spark |>
  formattable() |>
  as.htmlwidget() |>
  spk_add_deps()

GP_spark
```

This looks great! Some minor changes: can you add both the estimate and the se within parentheses for each line eg 2.2 (.54). And while this is great for working online, can you make it publication worthy (doesnt have search, sort functions)

## Question 14

14. Create model derived "predicted values" of the regression model based on original data (i.e fitted or Y-hat values). Use these to then plot the regression lines for each person plus an average regression line all within a single pane. To do this, I would use the marginal effects package, and then with the predicted values id graph them in ggplot using geom_line

I changed PID 518808 to 244 because it was messing with the labels.

```{r}
GP['PID'][GP['PID'] == 518806] <- 244

GP_predict <- predict(model_C)

GP$fitted_values <- GP_predict

GP |>
  ggplot(aes(x = wave, y = fitted_values)) +
  geom_line(aes(group = PID, color = PID)) +
  geom_smooth(color = "red")
```

I know you said I would have to calculate the average regression line separately, but I was thinking that geom_smooth() plotted a line that already did that. Is this not correct, and if so, what is the average line supposed to look like?

## Question 15

15. The regression, graphing, and table building all centered on a single model. Without necessarily doing it, how would you set it up to iterate through all possible regressions, graphs, and tables?

I thought to make a function utilizing the code from the previous questions but the function I did make is definitely flawed.

```{r}
allfunction <- function(GP, x_vars, y_vars, group_vars, GP_obs) {
  model <- lmList({{ y_vars }} ~ {{ x_vars }} | {{ group_vars }}, data = GP)
  
  frame <- data.frame(coef(model))

  GP |>
  ggplot(frame, aes(x = {{ x_vars }})) +
  geom_density()

  summary <- summary(model)
  
  GP_obs$intercept_se <- summary_C$coefficients[ , 2, 1]
  
  GP_obs$regression_se <- summary_C$coefficients[ , 2, 2]
  
  joinGP <- GP |>
    group_by({{ group_vars }}) |>
    select({{group_vars}}, {{ x_vars }}, {{ y_vars }})
  
  joinGP <- GP_join |>
    group_by({{ group_vars }}) |>
    summarize(
      TrendSparkline = spk_chr(
        {{y_vars}}, type = "line"
      )
    )
  
  joinGP <- joinGP |>
    full_join(GP_obs, join_by({{ group_vars }}))
  
  joinGP <- joinGP[, c("PID", "total_obs", "intercept_se", "regression_se",
                         "TrendSparkline")]
  
  GP_spark <- datatable(joinGP, escape = FALSE, filter = "top", 
                        options = list(paging = FALSE, 
                                       fnDrawCallback = htmlwidgets::JS(
                                         "
            function(){
            HTMLWidgets.staticRender();
            }
            "
                                       ))
  ) |>
    spk_add_deps()
}
```

I think the next step is definitely working with some loops and or purrr to get more comfortable. Writing functions is one major step, but the iterating through models is a separate, necessary beast.
